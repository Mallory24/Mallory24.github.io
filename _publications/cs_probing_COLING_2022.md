---
title: "Are Visual-Linguistic Models Commonsense Knowledge Bases?"
collection: publications
permalink: publication/cs_probing_COLING_2002
excerpt: '**Hsiu-Yu Yang** and Carina Silberer'
date: 2022-10-12
venue: 'The 29th International Conference on Computational Linguistics (COLING)'
paperurl: 'https://aclanthology.org/2022.coling-1.491/'

[//]: # (citation: 'Your Name, You. &#40;2009&#41;. &quot;Paper Title Number 1.&quot; <i>Journal 1</i>. 1&#40;1&#41;.')
---
Despite the recent success of pretrained language models as on-the-fly knowledge sources for various downstream tasks, they are shown to inadequately represent trivial common facts that vision typically captures. This limits their application to natural language understanding tasks that require commonsense knowledge. We seek to determine the capability of pretrained visual-linguistic models as knowledge sources on demand. To this end, we systematically compare language-only and visual-linguistic models in a zero-shot commonsense question answering inference task. We find that visual-linguistic models are highly promising regarding their benefit for text-only tasks on certain types of commonsense knowledge associated with the visual world. Surprisingly, this knowledge can be activated even when no visual input is given during inference, suggesting an effective multimodal fusion during pretraining. However, we reveal that there is still a huge space for improvement towards better cross-modal reasoning abilities and pretraining strategies for event understanding.
[//]: # ([Download paper here]&#40;http://academicpages.github.io/files/paper1.pdf&#41;)

[//]: # (Recommended citation: Your Name, You. &#40;2009&#41;. "Paper Title Number 1." <i>Journal 1</i>. 1&#40;1&#41;.)